% Indicate the main file. Must go at the beginning of the file.
% !TEX root = ../main.tex

%----------------------------------------------------------------------------------------
% CHAPTER TEMPLATE
%----------------------------------------------------------------------------------------


\chapter{Methodik} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
% SECTION 1
%----------------------------------------------------------------------------------------
Dieses Kapitel beschreibt die Methoden und Vorgehensweisen, zur Beantwortung der Forschungsfragen. Ziel ist es, aussagekräftige Metriken zu extrahieren und Zusammenhänge festzustellen. 
In diesem Kapitel werden die Metriken vorgestellt, die zur Beantwortung der Forschungsfragen notwendig sind. Es werden Definitionen und Berechnungen vorgestellt, um ein Verständnis für diese Metriken zu erlangen. Des Weiteren wird aufgezeigt, welche Metriken mittels Mining-Verfahren aus den Repositories extrahiert werden. Abschliessend wird die Datengrundlage vorgestellt, auf der die Analysen basieren.

\section{Metriken und Berechnungen Churn vs Latency}
\label{sec:AnalyseChurnvsLatency}
Wie in Kapitel \secref{sec:PullRequestDauer} beschrieben, finden sich in der Literatur unterschiedliche Aussagen zum Einfluss der Code-Grösse (Churn) auf die Dauer von Pull Requests (PR Latency). Während einige Studien einen Zusammenhang feststellen, bleiben andere Untersuchungen uneindeutig \parencite{hasan_understanding_2023}\parencite{kudrjavets_small_2022}.


Aus den im Kapitel \secref{sec:Metriken} Metriken werden die Metriken \textit{latency} und \textit{churn} berechnet. Die \textit{latency} wird anhand folgender Formel berechnet:
\begin{equation}
latency = closeTime - createTime
\end{equation}

Die Zusammensetzung des \textit{churn} ist wie folgt:
\begin{equation}
churn = additions + deletions
\end{equation}

Mittels der Rangkorrelation nach Spearman \parencite{noauthor_t-test_nodate} kann überprüft werden, ob ein Zusammenhang zwischen zwei Variablen besteht. Dabei verwendet Spearman die Rangplätze der Daten und nicht deren Wert. Der daraus resultierende Korrelationskoeffizient r\textsubscript{s} kann zwischen -1 und 1 liegen, wobei dieser bei eins ein starker positiver Zusammenhang und bei -1 ein starker negativer Zusammenhang zwischen den zwei Variablen besteht. Wenn der Korrelationskoeffizient bei null liegt, besteht keine Korrelation. \parencite{noauthor_t-test_nodate}  \\
Die Spearman-Formel \parencite{noauthor_t-test_nodate} stellt sich wie folgt zusammen: 
\begin{equation}
r_s = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
\end{equation}
\label{eqn:spearman}
\noindent\textbf{Legende:}
\begin{itemize}
  \item [$r_s$] Spearman-Rangkorrelationskoeffizient
  \item[$d_i$] Differenz der Ränge zwischen den zwei Variablen 
  \item[$n$] Anzahl der beobachteten Fälle
\end{itemize}


\section{Auswahl Metriken für Korrelationsanalyse}
\label{sec:MetrikenKorrelation}
Für die Korrelationsanalyse der Metriken zur Klärung von \fref{forschungsfrage6} müssen geeignete Metriken ausgewählt werden, die verschiedene Aspekte der Pull-Request-Dynamik abbilden. Soziale Faktoren wie die Erfahrung der Contributor oder die Dauer ihrer Mitarbeit an einem Projekt können jedoch nicht berücksichtigt werden, da solche Daten im Kontext der untersuchten Studenten-Repositories nicht verfügbar sind. Basierend auf der Analyse diverser wissenschaftlicher Literatur wurden folgende sieben Metriken ausgewählt:

Als \textbf{Latency} wird die Zeit zwischen der Erstellung eines Pull Requests und dessen Merge oder Schliessung genannt. Sie wird als Indikator für die Effizienz des Review-Prozesses genutzt und ist in der Literatur häufig als Schlüsselfaktor für die Messung der Geschwindigkeit von Code-Reviews und DevOps-Prozessen anerkannt. Eine hohe Latenz kann auf ineffiziente Reviews oder Verzögerungen hinweisen, weshalb sie in die Analyse aufgenommen wurde, um die Geschwindigkeit der Pull-Request-Bearbeitung zu messen und Zusammenhänge mit anderen Metriken wie \textit{Churn} und \textit{Comments} zu untersuchen. \parencite{yu_wait_2015}

Die Metrik \textbf{Churn} misst die Anzahl geänderter Codezeilen in einem Pull Request. Sie wird als Indikator für den Umfang und die Komplexität einer Änderung betrachtet und ist häufig mit längeren Review-Zeiten verbunden. Diese Metrik wurde ausgewählt, da grössere Änderungen im Code oft genauere Überprüfungen erfordern und somit die \textit{Latency} des Pull Requests beeinflussen können. \parencite{gousios_exploratory_2014}

Die Anzahl der \textbf{Commits} in einem Pull Request gibt an, wie viele Commits gemerdet werden sollen. Sie zeigt wie kleinschrittig oder iterativ ein Beitrag entwickelt wurde. Mehrere Commits deuten häufig darauf hin, dass der Pull Request mehrere Anpassungen durchlaufen hat, was zu einer längeren Review-Dauer führt. \parencite{zhang_pull_2022}

\textbf{Comments} messen die Anzahl der Kommentare, die während des Review-Prozesses als auch auf den Pull Request selber erstellt wurden. Eine höhere Anzahl an Kommentaren deutet auf intensivere Diskussionen hin, die den Review-Prozess verlängern können und die Wahrscheinlichkeit eines Merges verringern. Zudem ist eine hohe Zahl an Kommentaren oft mit einer höheren Qualität und Gründlichkeit des Reviews verbunden. \parencite{tsay_influence_2014}

Die Länge der Beschreibung eines Pull Requests (\textbf{Description Length}) ist ein Indikator für den Kontext, den der Entwickler für seine Änderungen liefert. Ausführliche Beschreibungen helfen den Reviewern, die Änderungen schneller zu verstehen, was den Review-Prozess beschleunigen kann. Kürzere Beschreibungen führen häufig zu mehr Rückfragen und somit zu einer längeren Review-Dauer. \parencite{zhang_pull_2022}

Neben der Anzahl geänderter Codezeilen gibt \textbf{Changed Files Count} an, wie viele Dateien von einer Änderung betroffen sind. Diese Metrik ist wichtig, da Änderungen, die viele Dateien betreffen, in der Regel komplexer sind und daher länger dauern, um überprüft und integriert zu werden. \parencite{tsay_influence_2014}

Die Metrik \textbf{Contributors} misst die Anzahl der verschiedenen Entwickler, die an einem Pull Request beteiligt sind. Ein höherer Beitrag von verschiedenen Entwicklern könnte auf eine intensivere Zusammenarbeit hinweisen und könnte mit der Komplexität und der Dauer des Review-Prozesses korrelieren.

\section{Benötigte Metriken}
Um die Forschungsfragen beantworten zu können, werden folgende Metriken benötigt. Die Begriffe selbst stammen direkt aus dem GitGauge Backend und sind daher in ihrer englischen Form aufgeführt \parencite{noauthor_repo-detectivesgitgauge-extractor-service_nodate}:
\label{sec:Metriken}
\begin{itemize}
    \item \textbf{Repository createTime:} Das Erstellungsdatum des Repository.
    \item \textbf{Pull Request createTime:} Das Erstellungsdatum des PullRequests.
    \item \textbf{Pull Request closeTime:} Die Schliesszeit(Closed / Merged) des Pull Requests.
    \item \textbf{Pull Request Additions:} Anzahl der hinzugefügten Codezeilen.
    \item \textbf{Pull Request Deletions:} Anzahl der gelöschten Codezeilen
      \item \textbf{Pull Request Commits:} Alle Commits die auf den Pull Requests gemacht werden.
     \begin{itemize}
        \item \textbf{CommittedDate:} Das Datum an welchem der Commit gemacht wird.
        \item \textbf{Author:} Die Person, welche den Commit erstellt hat.
    \end{itemize}
\end{itemize}

\section{Repository Mining}
Damit alle Analysen durchgeführt werden konnten, musste das Mining der Projekte erweitert werden. So wurde die GraphQL Abfrage für die Pull Requests erweitert, um zusätzlich den Churn als auch die Commits auf den Pull Requests zu speichern. Die Abfrage der Daten wurde durch einen neuen PullRequest Statistik API Controller ermöglicht, welcher sämtliche Daten der einzelnen Pull Requests, wie etwa der Churn, die einzelnen Commits des Pull Requests als auch die Anzahl geänderter Dateien zurückgibt. 

Für die einzelnen Analysen wurde jeweils ein neues Jupyter Notebook erstellt. Dieser ermöglichen eine schnelle und unkomplizierte (graphische) Auswertung der Daten. \parencite{noauthor_repo-detectivesba-metric-analysis-scripts_nodate}

\section{Datengrundlagen}
\label{sec:Datengrundlagen}
Als Datengrundlage dienen die Repositories aus dem ersten Projekt Racetrack, welches im zweiten Projektmodul stattfindet. Dieses dauert etwa vier Wochen, wobei die genaue Anzahl der Tage pro Klasse variieren kann. Das Grundgerüst des Sourcecodes wird in der Aufgabenstellung mitgeliefert, was die Vergleichbarkeit der Projekte erhöht. Es werden 7 Teilzeit- und 5 Vollzeitklassen aus den Jahren 2021 - 2024 analysiert. Insgesamt werden 71 Racetrack Repositories untersucht. 


